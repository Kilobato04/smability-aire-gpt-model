import pandas as pd
import xgboost as xgb
import glob
import os
import json
import numpy as np
import boto3
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# --- CONFIGURACI√ìN ---
# Ajusta esta ruta seg√∫n d√≥nde CodeBuild descomprima tu zip
RAW_DATA_DIR = '/tmp/dataset_final' 
STATIONS_FILE = os.path.join(os.path.dirname(__file__), 'raw_data/stationssimat.csv')

# Busca el archivo de edificios
BUILDINGS_FILE = 'capa_edificios_v2.json' 
if not os.path.exists(BUILDINGS_FILE):
    # Fallback por si la estructura de carpetas cambia en el contenedor
    BUILDINGS_FILE = '../capa_edificios_v2.json'

# Configuraci√≥n S3
S3_BUCKET = os.environ.get('S3_BUCKET', 'smability-data-lake')
S3_MODELS_DIR = 'models/' 

# --- LOS 5 FANT√ÅSTICOS ---
TARGETS_TO_TRAIN = ['o3', 'pm10', 'pm25', 'co', 'so2']

def upload_to_s3(local_file, s3_key):
    """Intenta subir el modelo a S3."""
    try:
        s3 = boto3.client('s3')
        print(f"‚¨ÜÔ∏è Subiendo {local_file} a s3://{S3_BUCKET}/{s3_key}...")
        s3.upload_file(local_file, S3_BUCKET, s3_key)
        print("‚úÖ Subida exitosa.")
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo subir a S3 autom√°ticamente: {e}")

def get_station_building_density(stations_df):
    """Asigna densidad urbana a las estaciones de entrenamiento."""
    if not os.path.exists(BUILDINGS_FILE):
        print(f"‚ö†Ô∏è No se encontr√≥ {BUILDINGS_FILE}. Usando densidad 0.")
        return np.zeros(len(stations_df))
    
    print(f"üèóÔ∏è Cruzando estaciones con {BUILDINGS_FILE}...")
    with open(BUILDINGS_FILE, 'r') as f:
        grid_data = json.load(f)
    
    buildings_df = pd.DataFrame(grid_data)
    grid_lats = buildings_df['lat'].values
    grid_lons = buildings_df['lon'].values
    grid_vols = buildings_df['building_vol'].values
    
    st_vols = []
    for _, row in stations_df.iterrows():
        # Distancia euclidiana r√°pida (Nearest Neighbor)
        dists = (grid_lats - row['lat'])**2 + (grid_lons - row['lon'])**2
        nearest_idx = np.argmin(dists)
        st_vols.append(grid_vols[nearest_idx])
        
    return np.array(st_vols)

def load_and_merge_data():
    print(f"üîÑ Buscando CSVs en {RAW_DATA_DIR}...")
    all_files = glob.glob(os.path.join(RAW_DATA_DIR, "*.csv"))
    
    df_list = []
    for filename in all_files:
        try:
            df = pd.read_csv(filename)
            # Normalizar columnas a min√∫sculas
            df.columns = [c.lower() for c in df.columns]
            df_list.append(df)
        except: pass
        
    if not df_list:
        raise Exception("‚ùå No se encontraron archivos CSV v√°lidos.")

    full_df = pd.concat(df_list, ignore_index=True)
    if 'station_id' in full_df.columns: full_df['station_id'] = full_df['station_id'].astype(str)

    # Pivot din√°mico: Convierte par√°metros (co, so2, etc) en columnas
    print("üîÑ Pivoteando tabla maestra...")
    pivot_df = full_df.pivot_table(
        index=['date', 'hour', 'station_id'], 
        columns='parameter', 
        values='value'
    ).reset_index()
    
    # Estandarizar nombres de columnas a min√∫sculas
    pivot_df.columns = [c.lower() for c in pivot_df.columns]

    if os.path.exists(STATIONS_FILE):
        print(f"üó∫Ô∏è Cruzando con cat√°logo de estaciones...")
        stations_df = pd.read_csv(STATIONS_FILE)
        stations_df.columns = [c.lower() for c in stations_df.columns]
        
        # Normalizaci√≥n de nombres de estaci√≥n
        if 'station_id' in stations_df.columns: stations_df = stations_df.drop(columns=['station_id'])
        if 'station_code' in stations_df.columns: stations_df = stations_df.rename(columns={'station_code': 'station_id'})
        
        # Inyecci√≥n de Variable F√≠sica: Edificios
        stations_df['building_vol'] = get_station_building_density(stations_df)
        print(f"   Densidad Urbana Promedio: {stations_df['building_vol'].mean():.1f} m3/km2")

        cols_needed = ['station_id', 'lat', 'lon', 'altitude', 'building_vol']
        stations_subset = stations_df[cols_needed].copy()
        
        # Limpieza de strings para el merge
        stations_subset['station_id'] = stations_subset['station_id'].astype(str).str.strip()
        pivot_df['station_id'] = pivot_df['station_id'].astype(str).str.strip()
        
        pivot_df = pd.merge(pivot_df, stations_subset, on='station_id', how='inner')
    else:
        print("‚ö†Ô∏è No se encontr√≥ stationssimat.csv. El modelo no tendr√° lat/lon/edificios precisos.")
    
    return pivot_df

def feature_engineering(df, target):
    if target not in df.columns: 
        print(f"‚ö†Ô∏è Target '{target}' no encontrado en el dataset. Saltando.")
        return None, None
        
    df_clean = df.dropna(subset=[target]).copy()
    df_clean['date'] = pd.to_datetime(df_clean['date'])
    
    # Feature Engineering Temporal
    df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)
    df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)
    df_clean['month'] = df_clean['date'].dt.month
    df_clean['month_sin'] = np.sin(2 * np.pi * df_clean['month'] / 12)
    df_clean['month_cos'] = np.cos(2 * np.pi * df_clean['month'] / 12)
    
    # Relleno de variables meteorol√≥gicas (Promedios si faltan)
    for col in ['tmp', 'rh', 'wsp', 'wdr']:
        if col in df_clean.columns: df_clean[col] = df_clean[col].fillna(df_clean[col].mean())
        else: df_clean[col] = 0 
    
    if 'altitude' in df_clean.columns: df_clean['altitude'] = df_clean['altitude'].fillna(2240)
    if 'building_vol' in df_clean.columns: df_clean['building_vol'] = df_clean['building_vol'].fillna(0)
            
    df_clean['station_numeric'] = df_clean['station_id'].astype('category').cat.codes
    return df_clean, target

def train():
    try:
        master_df = load_and_merge_data()
        print(f"üìã Columnas disponibles: {list(master_df.columns)}")

        for target in TARGETS_TO_TRAIN:
            print("\n" + "="*60)
            print(f"üß† ENTRENANDO: {target.upper()}")
            
            df_target, target_col = feature_engineering(master_df, target)
            if df_target is None or len(df_target) < 100: 
                print(f"‚ö†Ô∏è Datos insuficientes para {target}. Saltando.")
                continue

            POSSIBLE_FEATURES = [
                'lat', 'lon', 'altitude', 'building_vol', 
                'station_numeric', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 
                'tmp', 'rh', 'wsp', 'wdr'
            ]
            FEATURES = [f for f in POSSIBLE_FEATURES if f in df_target.columns]
            print(f"   Features usados: {FEATURES}")
            
            X = df_target[FEATURES]
            y = df_target[target_col]
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Hiperpar√°metros XGBoost (Optimizados para generalizaci√≥n)
            model = xgb.XGBRegressor(n_estimators=150, learning_rate=0.05, max_depth=7, n_jobs=-1)
            model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
            
            rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test)))
            print(f"‚úÖ Modelo {target.upper()} Generado - RMSE: {rmse:.2f}")
            
            output_filename = f"model_{target}.json"
            model.save_model(output_filename)
            
            if S3_BUCKET:
                upload_to_s3(output_filename, f"{S3_MODELS_DIR}{output_filename}")

    except Exception as e:
        print(f"‚ùå Error cr√≠tico en el proceso de entrenamiento: {e}")
        exit(1)

if __name__ == "__main__":
    train()
